---
title: 'DATA643 Final Project: Yelp Recommendation System'
author: "Yun Mai, Kelly Shaffer"
date: "July 16, 2017"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Restaurant Recommendation System based on Yelp data
 
### 1.Introduction
 
As modern consumers, we greatly benefit from restaurant recommendation applications. It is so convenient to get a list of restaurants that match our preferences without much clicking, comparing, and browsing through a long list of reviews for each single business.
 
In this project, we want to apply the algorithms to develop predictive models learned from the DATA643 course "of "Current Topic of Data Science - Recommendation System"" to build a restaurant recommendation system that suggests the most suitable restaurant for users. If time permits, we will build an application.
 
### 2.Motivation
 
It is very common that we hang out with families, friends, and coworkers when comes to lunch or dinner time. As the users of recommendation applications, we care more about how we will like a restaurant. We will tend to have happier experiences when the prediction of the recommendation system is as good as what it says. As there is a completed and big data set of user and restaurants reviews, we want to see whether we can use the latest techniques to make good predictions. In the data set, there are not only reviews but also relevant information of users and restaurants that allow us to do more complicated computation, which might lead to the construction of a better model.
 
### 3.Dataset 
In this project, we will use a Yelp Dataset Challenge round 9 from yelp website. The dataset has 4.1M reviews and 947K tips by 1M users for 144K businesses; 1.1M business attributes, e.g. hours, parking availability, ambience; and aggregated check-ins over time for each of the 125K businesses. The data includes diverse sets of cities: Edinburgh in U.K.; Karlsruhe in Germany; Montreal and Waterloo in Canada; Pittsburgh, Charlotte, Urbana-Champaign, Phoenix, Las Vagas, Madison, and Cleveland in U.S. 

```{r,eval=F}
install.packages("jsonlite",repos='http://cran.us.r-project.org')
devtools::install_github("sailthru/tidyjson")
install.packages("doParallel")
install.packages(('BBmisc'))
```

Load packages
```{r}
suppressWarnings(suppressMessages(library(jsonlite)))
suppressWarnings(suppressMessages(library(tidyjson)))
suppressWarnings(suppressMessages(library(plyr)))
suppressWarnings(suppressMessages(library(dplyr)))
suppressWarnings(suppressMessages(library(recommenderlab)))
suppressWarnings(suppressMessages(library(knitr)))
suppressWarnings(suppressMessages(library(tidyr)))
suppressWarnings(suppressMessages(library(ggplot2)))

# user-item matrix
suppressWarnings(suppressMessages(library(stringi)))
suppressWarnings(suppressMessages(library(Matrix)))

```

#### 3.1 Load the pre-processed data 
 

```{r,echo=F,eval=F}
#load the pre-processed data
business <- readRDS("E:/YM_work/CUNY_643/DATA643_final_project/Yelp_rs/business.Rds")

user <- readRDS("E:/YM_work/CUNY_643/DATA643_final_project/Yelp_rs/user.Rds")

review <- read.csv("E:/YM_work/CUNY_643/DATA643_final_project/Yelp_rs/review_1.csv")

for (i in c(2:27)){
  a<- paste0(cat('"'),'E:/YM_work/CUNY_643/DATA643_final_project/Yelp_rs/review_',i,'.csv',cat('"'))
  review_1 <- read.csv(a)
  review <- rbind(review, review_1)
}

#write.csv(review[1:300000,],"review2M_1.csv")
#write.csv(review[300001:600000,],"review2M_2.csv")
#write.csv(review[600001:900000,],"review2M_3.csv")
#write.csv(review[900001:1200000,],"review2M_4.csv")
#write.csv(review[1200001:1500000,],"review2M_5.csv")
#write.csv(review[1500001:1800000,],"review2M_6.csv")
#write.csv(review[1800001:1882000,],"review2M_7.csv")

```



```{r,echo=F,eval=F}
review2M <- read.csv("https://raw.githubusercontent.com/YunMai-SPS/DA643/master/DA643_final_project/review2M_1.csv")

for (i in c(2:7)){
  a<- paste0(cat('"'),'https://raw.githubusercontent.com/YunMai-SPS/DA643/master/DA643_final_project/review2M_',i,'.csv',cat('"'))
  review_1 <- read.csv(a)
  review2M <- rbind(review2M, review_1)
}

business <- readRDS("E:/YM_work/CUNY_643/DATA643_final_project/Yelp_rs/business.Rds")

user <- readRDS("E:/YM_work/CUNY_643/DATA643_final_project/Yelp_rs/user.Rds")

rating <- merge(review[,-1],business[,c(1,2)], by.x='business_id',by.y='business_id')
rating <- merge(rating,user[,c(1,2)], by.x='user_id',by.y='user_id')
colnames(rating)[which(names(rating) == "name.x")] <- "restaurant"
colnames(rating)[which(names(rating) == "name.y")] <- "user"
# remove duplicated data
rating_ndup <- rating[!duplicated(rating[c('user','restaurant')]),]

#write.csv(rating_ndup[1:200000,],"rating_1.csv")
#write.csv(rating_ndup[200001:400000,],"rating_2.csv")
#write.csv(rating_ndup[400001:600000,],"rating_3.csv")
#write.csv(rating_ndup[600001:800000,],"rating_4.csv")
#write.csv(rating_ndup[800001:1000000,],"rating_5.csv")
#write.csv(rating_ndup[1200001:1400000,],"rating_6.csv")
#write.csv(rating_ndup[1400001:1609143,],"rating_7.csv")

# select variants
business<- business[,c("business_id","name","neighborhood", "address","city" ,"state",  "postal_code", "latitude", "longitude","stars", "review_count")]
write.csv(business,"business.csv")
user<-user[,c("user_id", "name","review_count","useful", "funny", "cool", "fans", "average_stars")]

user_1 <- user[1:250000,]
user_2 <- user[250001:500000,]
user_3 <- user[500001:750000,]
user_4 <- user[750001:1029432,]

write.csv(user_1,"user_1.csv")
write.csv(user_2,"user_2.csv")
write.csv(user_3,"user_3.csv")
write.csv(user_4,"user_4.csv")
```

#### 3.1 Explore the data 

Load the pre-processed data
```{r}
# read data from Github repository
business<- read.csv("https://raw.githubusercontent.com/YunMai-SPS/DA643/master/DA643_final_project/business.csv")

user <- read.csv("https://raw.githubusercontent.com/YunMai-SPS/DA643/master/DA643_final_project/user_1.csv")
for (i in c(2:4)){
  a<- paste0(cat('"'),'https://raw.githubusercontent.com/YunMai-SPS/DA643/master/DA643_final_project/user_',i,'.csv',cat('"'))
  user_1 <- read.csv(a)
  user <- rbind(user, user_1)
}

rating <- read.csv("https://raw.githubusercontent.com/YunMai-SPS/DA643/master/DA643_final_project/rating_1.csv")

for (i in c(2:7)){
  a<- paste0(cat('"'),'https://raw.githubusercontent.com/YunMai-SPS/DA643/master/DA643_final_project/rating_',i,'.csv',cat('"'))
  rating_1 <- read.csv(a)
  rating <- rbind(rating, rating_1)
}

rating_copy <- rating
```


** View the data**
```{r}
#rearrange the column
rating <- rating[,c("restaurant", "business_id", "user", "user_id","stars", "useful", "funny", "cool" ,"document.id")]

kable(head(rating,n=5))
# convert ratings data to realRatingMatrix for implement of recommenderlab package

#check how many user and how many restaurant we have 
#length(unique(rating[,"user"]))  [1] 63081
#length(unique(rating[,"restaurant"])) [1] 65432


#builkd the user-item matrix
udf <- data.frame(user_No= seq(63081),user= unique(rating[,"user"]))
idf <- data.frame(restaurant_No= seq(65432),restaurant=unique(rating[,"restaurant"]))

rating <- merge(rating,udf,by.x='user',by.y='user')
rating <- merge(rating,idf,by.x='restaurant',by.y='restaurant')

rating_mx <- sparseMatrix(
  i =  rating$user_No, 
  j =  rating$restaurant_No, 
  x = rating$stars, 
  dimnames = list(levels(rating$user_No), levels(rating$restaurant_No))
)

mx <- as(rating_mx,"realRatingMatrix")

#Normalize by subtracting the row mean from all ratings in the row
mx_n <- normalize(mx)

image(mx, main = "Yelp restarurant reviews Data")
image(mx_n, main = "Normalized Yelp restarurant reviews Data")
```


** Statistics of ratings data**
```{r}
# use visualize_ratings function from SVDApproximation to visualize statistics for all ratings: item count of different ratings,item histogram of users' average ratings, item histogram of items' average ratings, item histogram of number of rated items by user, item histogram of number of scores items have

#distribution of ratings
rating_frq <- as.data.frame(table(rating$stars))

ggplot(rating_frq,aes(Var1,Freq)) +   
  geom_bar(aes(fill = Var1), position = "dodge", stat="identity",fill="palegreen")+ labs(x = "Stars")

#calculate average reviews for each restaurant
business_mean <- data.frame(restaurant = idf$restaurant, average_stars=colMeans(mx))

par(mfrow=c(2,2))

ggplot(user,aes(review_count)) +
  geom_histogram(binwidth = 0.05,col='red',fill="plum") + coord_cartesian(ylim=c(0,12000)) + labs(x = "User Average Reviews")+geom_vline(xintercept = mean(user$review_count),col='blue',size=1)

ggplot(business,aes(review_count)) +
  geom_histogram(binwidth = 0.05,col='blue',fill="sandybrown") + coord_cartesian(ylim=c(0,7000)) + labs(x = "Restaurant Average Reviews")+geom_vline(xintercept = mean(business$review_count),col='red',size=1)

ggplot(user,aes(average_stars)) +
  geom_histogram(binwidth = 0.03,fill="plum")  + labs(x = "Mean of Reviews User Gives")
ggplot(business_mean,aes(average_stars)) +
  geom_histogram(binwidth = 0.03,fill="sandybrown") + labs(x = "Mean of Reviews Restaurant Has")
```


```{r}



```



### Algorithms
In this project, we will use a Yelp Dataset Challenge round 9 from yelp website. The dataset has 4.1M reviews and 947K tips by 1M users for 144K businesses; 1.1M business attributes, e.g. hours, parking availability, ambience; and aggregated check-ins over time for each of the 125K businesses. The data includes diverse sets of cities: Edinburgh in U.K.; Karlsruhe in Germany; Montreal and Waterloo in Canada; Pittsburgh, Charlotte, Urbana-Champaign, Phoenix, Las Vagas, Madison, and Cleveland in U.S.
 
In the Yelp dataset there is more information other than only ratings, so we can not only use content-based algorithm but also collaborative filtering algorithms. Location of the restaurant is an important factor to do the recommendation, so the location will be considered so the similarity between the distance and similarity between user/items will be combined. Other algorithms like alternative linear squares and singular value decomposition will also be used to build the prediction models.
 
Because there are three criteria in reviews: funny, useful, and cool, the rating will be calculated as follows:

$$ R: Users \times Items \to R_{0} \times R_{1} \times ...R_{k}$$

$R_{0}$ is the set of possible overall rating values, and $R_{i}$ represents the possible rating values for each individual criterion i (i = 1,..,k), typically on some numeric scale.
 
The prediction results of single-criteria collaborative filtering algorithm and multi-criteria collaborative filtering algorithms will be compared to decide which approach is better.
 
The implementation and evaluation will be performed in R and Apache Spark. At last, if time permits, an application will be built with the Shiny package.
 
### Reference:
1. Blanca Vargas-Govea, Gabriel González-Serna, Rafael Ponce-Medellín. Effects of relevant contextual features in the performance of a restaurant recommender system.CARS,( 2011)
2. Mengqi Yu, Meng Xue, Wenjia Ouyang. Restaurants Review Star Prediction for Yelp Dataset.Conference Proceedings (2015).
3. Gediminas Adomavicius, YoungOk Kwon. New Recommendation Techniques for Multi-Criteria Rating Systems. IEEE Intelligent Systems 22-3 (2017).
4. Jun Zeng, Feng Li, Haiyang Liu, Junhao Wen, Sachio Hirokawa. A Restaurant Recommender System Based on User Preference and Location in Mobile Environment. Advanced Applied Informatics (IIAI-AAI), 2016 5th IIAI International Congress.

```{r}

```